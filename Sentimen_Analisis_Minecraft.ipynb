{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##install library"
      ],
      "metadata": {
        "id": "MyGTZGorZTAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scipy gensim tensorflow scikit-learn nltk imblearn emoji PySastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abyyY3Ex-L_E",
        "outputId": "75e717c4-ebe0-402b-b090-e561488b46be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.12/dist-packages (0.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (2.14.1)\n",
            "Requirement already satisfied: PySastrawi in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (from imblearn) (0.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## impor library yang dibutuhkan"
      ],
      "metadata": {
        "id": "vYgsvMdAZHY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "devHiP_Q9E6L",
        "outputId": "c24b0903-2ee7-453b-cf02-d32d94ac60c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from gensim.models import Word2Vec\n",
        "from Sastrawi.Stemmer import StemmerFactory\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "slang_df = pd.read_csv('/content/kamus_slang.csv')\n",
        "slang_dict = dict(zip(slang_df['slang'], slang_df['formal']))"
      ],
      "metadata": {
        "id": "_sUI1hbwC7KI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##import csv\n"
      ],
      "metadata": {
        "id": "PdQO-Otjd40h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/hasil_ulasan_minecraft.csv')\n",
        "print(\"Jumlah data:\", len(df))\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt98v3Ws9Gzs",
        "outputId": "4d29e3ed-f660-481e-940d-27fbb0a22199"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah data: 10000\n",
            "                               reviewId         userName  \\\n",
            "0  36773bdc-c113-44ce-9eaa-78070dc31a5e  Pengguna Google   \n",
            "1  dbddaf78-3ed0-4fa2-a957-1f58f4ed09c7  Pengguna Google   \n",
            "2  7d3a251d-a7fe-4752-92a3-85cd18a37c48  Pengguna Google   \n",
            "3  600e6ed6-c92d-4fc2-b91a-34569cc0fea1  Pengguna Google   \n",
            "4  8bef8613-43c8-4ff1-80ee-b5ffe1df2a66  Pengguna Google   \n",
            "\n",
            "                                           userImage  \\\n",
            "0  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
            "1  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
            "2  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
            "3  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
            "4  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
            "\n",
            "                                             content  score  thumbsUpCount  \\\n",
            "0                m malas menjawab tapi game kesukaan      5              0   \n",
            "1  game nya bagus banget karna ada shader nya tan...      5              0   \n",
            "2                           mojang kamu islam bukan?      5              0   \n",
            "3                            boring banget tapi seru      5              0   \n",
            "4                                          nice game      5              0   \n",
            "\n",
            "  reviewCreatedVersion                   at  replyContent  repliedAt  \\\n",
            "0                  NaN  2025-09-17 12:31:39           NaN        NaN   \n",
            "1           1.21.101.1  2025-09-17 12:11:07           NaN        NaN   \n",
            "2           1.21.101.1  2025-09-17 12:10:38           NaN        NaN   \n",
            "3           1.21.101.1  2025-09-17 11:38:07           NaN        NaN   \n",
            "4           1.21.101.1  2025-09-17 11:20:24           NaN        NaN   \n",
            "\n",
            "   appVersion  \n",
            "0         NaN  \n",
            "1  1.21.101.1  \n",
            "2  1.21.101.1  \n",
            "3  1.21.101.1  \n",
            "4  1.21.101.1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##labeling dan cleaning text"
      ],
      "metadata": {
        "id": "bpQnohcdd-xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_sentiment(score):\n",
        "    if score <= 2: return 'negatif'\n",
        "    elif score == 3: return 'netral'\n",
        "    else: return 'positif'\n",
        "\n",
        "df['sentiment'] = df['score'].apply(label_sentiment)\n",
        "print(\"Distribusi awal:\\n\", df['sentiment'].value_counts().to_string())\n",
        "\n",
        "stop_words = set(stopwords.words('indonesian')) | {'dan', 'yang', 'di', 'ke', 'nya', 'ini', 'itu'}\n",
        "\n",
        "def bersihin_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    words = text.split()\n",
        "    text = ' '.join(slang_dict.get(word, word) for word in words if word not in stop_words or word in ['oke', 'bagus', 'top'])\n",
        "    return stemmer.stem(text)\n",
        "\n",
        "df['cleaned_content'] = df['content'].apply(bersihin_text)\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=10000, stop_words=list(stop_words), ngram_range=(1, 2))\n",
        "X_tfidf = tfidf.fit_transform(df['cleaned_content']).toarray()\n",
        "y = pd.get_dummies(df['sentiment']).values\n",
        "smote = SMOTE(random_state=42)\n",
        "X_tfidf_smote, y_smote = smote.fit_resample(X_tfidf, np.argmax(y, axis=1))\n",
        "y_smote = pd.get_dummies(y_smote).values\n",
        "df_balanced = pd.DataFrame({'cleaned_content': [' '.join(doc) for doc in tfidf.inverse_transform(X_tfidf_smote)], 'sentiment': np.argmax(y_smote, axis=1)})\n",
        "df_balanced['sentiment'] = df_balanced['sentiment'].map({0: 'negatif', 1: 'netral', 2: 'positif'})\n",
        "print(f\"Jumlah data setelah SMOTE: {len(df_balanced)}\")\n",
        "print(\"Distribusi setelah SMOTE:\\n\", df_balanced['sentiment'].value_counts().to_string())\n",
        "\n",
        "def Evaluasi_Model(y_true, y_pred, set_name=\"\"):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"\\nAkurasi {set_name}: {accuracy * 100:.2f}%\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['negatif', 'netral', 'positif']))\n",
        "    return accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzsWeAfs9G7B",
        "outputId": "ee0217c7-68a8-467a-8a3c-42c3d85f7c7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi awal:\n",
            " sentiment\n",
            "positif    7494\n",
            "negatif    1937\n",
            "netral      569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah data setelah SMOTE: 22482\n",
            "Distribusi setelah SMOTE:\n",
            " sentiment\n",
            "positif    7494\n",
            "negatif    7494\n",
            "netral     7494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##define model"
      ],
      "metadata": {
        "id": "ce5v2-UueDpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dense_tfidf(X, y, test_size=0.2, epochs=25, batch_size=64, name=\"Dense + TF-IDF\"):\n",
        "    print(f\"\\n=== Skema: {name} (Test size={test_size}) ===\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    model = Sequential([\n",
        "        Input(shape=(X.shape[1],)),\n",
        "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        Dropout(0.4),\n",
        "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        Dropout(0.4),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, lr_scheduler],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    y_pred_train = np.argmax(model.predict(X_train), axis=1)\n",
        "    y_pred_test = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_train_cat = np.argmax(y_train, axis=1)\n",
        "    y_test_cat = np.argmax(y_test, axis=1)\n",
        "\n",
        "    train_acc = Evaluasi_Model(y_train_cat, y_pred_train, \"Training\")\n",
        "    test_acc = Evaluasi_Model(y_test_cat, y_pred_test, \"Testing\")\n",
        "\n",
        "    return model, train_acc, test_acc\n",
        "\n",
        "\n",
        "def train_lstm_word2vec(df, y, max_words=10000, embedding_dim=200, max_len=100, test_size=0.2, epochs=25, batch_size=64):\n",
        "    print(f\"\\n=== Skema: LSTM + Word2Vec (Test size={test_size}) ===\")\n",
        "\n",
        "    sentences = [text.split() for text in df['cleaned_content']]\n",
        "    w2v_model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4, epochs=20)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=max_words)\n",
        "    tokenizer.fit_on_texts(df['cleaned_content'])\n",
        "    X_seq = tokenizer.texts_to_sequences(df['cleaned_content'])\n",
        "    X_pad = pad_sequences(X_seq, maxlen=max_len)\n",
        "\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i < max_words and word in w2v_model.wv:\n",
        "            embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True),\n",
        "        Bidirectional(LSTM(256, return_sequences=True, kernel_regularizer=l2(0.005))),\n",
        "        LSTM(128),\n",
        "        Dropout(0.5),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.005)),\n",
        "        Dropout(0.5),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, lr_scheduler],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    y_pred_train = np.argmax(model.predict(X_train), axis=1)\n",
        "    y_pred_test = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_train_cat = np.argmax(y_train, axis=1)\n",
        "    y_test_cat = np.argmax(y_test, axis=1)\n",
        "\n",
        "    train_acc = Evaluasi_Model(y_train_cat, y_pred_train, \"Training\")\n",
        "    test_acc = Evaluasi_Model(y_test_cat, y_pred_test, \"Testing\")\n",
        "\n",
        "    return model, train_acc, test_acc, tokenizer"
      ],
      "metadata": {
        "id": "qTTvO7Do9HJG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##traning model"
      ],
      "metadata": {
        "id": "bDuPyvSBeGyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skema 1\n",
        "model1, train_acc1, test_acc1 = train_dense_tfidf(X_tfidf_smote, y_smote, test_size=0.2, name=\"Dense + TF-IDF 80/20\")\n",
        "\n",
        "# Skema 2\n",
        "model2, train_acc2, test_acc2, tokenizer = train_lstm_word2vec(df_balanced, y_smote, max_len=100)\n",
        "\n",
        "# Skema 3\n",
        "model3, train_acc3, test_acc3 = train_dense_tfidf(X_tfidf_smote, y_smote, test_size=0.3, name=\"Dense + TF-IDF 70/30\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkGe_oIN7HP_",
        "outputId": "69ebaf55-899d-4758-a1fe-2de160ef511d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Skema: Dense + TF-IDF 80/20 (Test size=0.2) ===\n",
            "Epoch 1/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.5696 - loss: 1.3671 - val_accuracy: 0.8060 - val_loss: 0.7436 - learning_rate: 5.0000e-04\n",
            "Epoch 2/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8469 - loss: 0.6598 - val_accuracy: 0.8555 - val_loss: 0.6824 - learning_rate: 5.0000e-04\n",
            "Epoch 3/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8910 - loss: 0.5706 - val_accuracy: 0.8416 - val_loss: 0.6593 - learning_rate: 5.0000e-04\n",
            "Epoch 4/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9038 - loss: 0.5360 - val_accuracy: 0.8749 - val_loss: 0.6401 - learning_rate: 5.0000e-04\n",
            "Epoch 5/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9162 - loss: 0.5135 - val_accuracy: 0.8788 - val_loss: 0.6243 - learning_rate: 5.0000e-04\n",
            "Epoch 6/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9253 - loss: 0.4886 - val_accuracy: 0.8838 - val_loss: 0.6188 - learning_rate: 5.0000e-04\n",
            "Epoch 7/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9255 - loss: 0.4721 - val_accuracy: 0.8838 - val_loss: 0.6228 - learning_rate: 5.0000e-04\n",
            "Epoch 8/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9343 - loss: 0.4462 - val_accuracy: 0.8805 - val_loss: 0.5999 - learning_rate: 5.0000e-04\n",
            "Epoch 9/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9322 - loss: 0.4413 - val_accuracy: 0.8883 - val_loss: 0.5928 - learning_rate: 5.0000e-04\n",
            "Epoch 10/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9308 - loss: 0.4370 - val_accuracy: 0.8822 - val_loss: 0.5899 - learning_rate: 5.0000e-04\n",
            "Epoch 11/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9336 - loss: 0.4274 - val_accuracy: 0.8899 - val_loss: 0.5844 - learning_rate: 5.0000e-04\n",
            "Epoch 12/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9369 - loss: 0.4170 - val_accuracy: 0.8855 - val_loss: 0.5638 - learning_rate: 5.0000e-04\n",
            "Epoch 13/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9337 - loss: 0.4170 - val_accuracy: 0.8827 - val_loss: 0.5748 - learning_rate: 5.0000e-04\n",
            "Epoch 14/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9372 - loss: 0.4029 - val_accuracy: 0.8833 - val_loss: 0.5479 - learning_rate: 5.0000e-04\n",
            "Epoch 15/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9389 - loss: 0.3983 - val_accuracy: 0.8933 - val_loss: 0.5532 - learning_rate: 5.0000e-04\n",
            "Epoch 16/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9421 - loss: 0.3859 - val_accuracy: 0.8755 - val_loss: 0.5581 - learning_rate: 5.0000e-04\n",
            "Epoch 17/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9480 - loss: 0.3635 - val_accuracy: 0.8877 - val_loss: 0.5200 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9495 - loss: 0.3333 - val_accuracy: 0.8894 - val_loss: 0.5033 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9497 - loss: 0.3252 - val_accuracy: 0.8888 - val_loss: 0.4945 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9543 - loss: 0.3048 - val_accuracy: 0.8855 - val_loss: 0.4921 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9533 - loss: 0.2998 - val_accuracy: 0.8888 - val_loss: 0.4894 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9575 - loss: 0.2955 - val_accuracy: 0.8872 - val_loss: 0.4746 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9504 - loss: 0.2995 - val_accuracy: 0.8838 - val_loss: 0.4816 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9541 - loss: 0.2876 - val_accuracy: 0.8899 - val_loss: 0.4687 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9536 - loss: 0.2898 - val_accuracy: 0.8905 - val_loss: 0.4693 - learning_rate: 1.0000e-04\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "Akurasi Training: 95.10%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.97      0.93      0.95      5995\n",
            "      netral       0.92      0.98      0.95      6051\n",
            "     positif       0.97      0.94      0.95      5939\n",
            "\n",
            "    accuracy                           0.95     17985\n",
            "   macro avg       0.95      0.95      0.95     17985\n",
            "weighted avg       0.95      0.95      0.95     17985\n",
            "\n",
            "\n",
            "Akurasi Testing: 89.79%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.89      0.91      0.90      1499\n",
            "      netral       0.85      0.98      0.91      1443\n",
            "     positif       0.96      0.81      0.88      1555\n",
            "\n",
            "    accuracy                           0.90      4497\n",
            "   macro avg       0.90      0.90      0.90      4497\n",
            "weighted avg       0.90      0.90      0.90      4497\n",
            "\n",
            "\n",
            "=== Skema: LSTM + Word2Vec (Test size=0.2) ===\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.5691 - loss: 3.1051 - val_accuracy: 0.6504 - val_loss: 1.2379 - learning_rate: 5.0000e-04\n",
            "Epoch 2/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - accuracy: 0.6646 - loss: 1.1431 - val_accuracy: 0.7115 - val_loss: 0.9077 - learning_rate: 5.0000e-04\n",
            "Epoch 3/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.7295 - loss: 0.8652 - val_accuracy: 0.7543 - val_loss: 0.7763 - learning_rate: 5.0000e-04\n",
            "Epoch 4/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7943 - loss: 0.6996 - val_accuracy: 0.7966 - val_loss: 0.6699 - learning_rate: 5.0000e-04\n",
            "Epoch 5/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.8350 - loss: 0.5878 - val_accuracy: 0.8266 - val_loss: 0.6030 - learning_rate: 5.0000e-04\n",
            "Epoch 6/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - accuracy: 0.8604 - loss: 0.5219 - val_accuracy: 0.8416 - val_loss: 0.5617 - learning_rate: 5.0000e-04\n",
            "Epoch 7/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8722 - loss: 0.4877 - val_accuracy: 0.8544 - val_loss: 0.5137 - learning_rate: 5.0000e-04\n",
            "Epoch 8/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - accuracy: 0.8906 - loss: 0.4222 - val_accuracy: 0.8694 - val_loss: 0.4770 - learning_rate: 5.0000e-04\n",
            "Epoch 9/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.8971 - loss: 0.4135 - val_accuracy: 0.8705 - val_loss: 0.4587 - learning_rate: 5.0000e-04\n",
            "Epoch 10/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.8987 - loss: 0.3948 - val_accuracy: 0.8694 - val_loss: 0.4951 - learning_rate: 5.0000e-04\n",
            "Epoch 11/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.9042 - loss: 0.3856 - val_accuracy: 0.8733 - val_loss: 0.4768 - learning_rate: 5.0000e-04\n",
            "Epoch 12/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9220 - loss: 0.3327 - val_accuracy: 0.8922 - val_loss: 0.4046 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.9366 - loss: 0.2851 - val_accuracy: 0.8983 - val_loss: 0.3915 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9369 - loss: 0.2639 - val_accuracy: 0.8988 - val_loss: 0.3956 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.9414 - loss: 0.2533 - val_accuracy: 0.9027 - val_loss: 0.3906 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.9421 - loss: 0.2429 - val_accuracy: 0.9038 - val_loss: 0.3828 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9411 - loss: 0.2379 - val_accuracy: 0.9083 - val_loss: 0.3710 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.9467 - loss: 0.2310 - val_accuracy: 0.9033 - val_loss: 0.3815 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9429 - loss: 0.2298 - val_accuracy: 0.8999 - val_loss: 0.4033 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - accuracy: 0.9507 - loss: 0.2075 - val_accuracy: 0.9127 - val_loss: 0.3637 - learning_rate: 2.0000e-05\n",
            "Epoch 21/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - accuracy: 0.9547 - loss: 0.2011 - val_accuracy: 0.9138 - val_loss: 0.3670 - learning_rate: 2.0000e-05\n",
            "Epoch 22/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.9526 - loss: 0.1990 - val_accuracy: 0.9133 - val_loss: 0.3654 - learning_rate: 2.0000e-05\n",
            "Epoch 23/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9581 - loss: 0.1849 - val_accuracy: 0.9122 - val_loss: 0.3699 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9600 - loss: 0.1871 - val_accuracy: 0.9116 - val_loss: 0.3705 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.9579 - loss: 0.1885 - val_accuracy: 0.9133 - val_loss: 0.3694 - learning_rate: 1.0000e-05\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Akurasi Training: 95.11%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.94      0.95      0.95      5995\n",
            "      netral       0.98      0.96      0.97      6051\n",
            "     positif       0.93      0.94      0.94      5939\n",
            "\n",
            "    accuracy                           0.95     17985\n",
            "   macro avg       0.95      0.95      0.95     17985\n",
            "weighted avg       0.95      0.95      0.95     17985\n",
            "\n",
            "\n",
            "Akurasi Testing: 91.84%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.89      0.93      0.91      1499\n",
            "      netral       0.95      0.97      0.96      1443\n",
            "     positif       0.92      0.86      0.89      1555\n",
            "\n",
            "    accuracy                           0.92      4497\n",
            "   macro avg       0.92      0.92      0.92      4497\n",
            "weighted avg       0.92      0.92      0.92      4497\n",
            "\n",
            "\n",
            "=== Skema: Dense + TF-IDF 70/30 (Test size=0.3) ===\n",
            "Epoch 1/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.5677 - loss: 1.4145 - val_accuracy: 0.7802 - val_loss: 0.7749 - learning_rate: 5.0000e-04\n",
            "Epoch 2/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8372 - loss: 0.6865 - val_accuracy: 0.8463 - val_loss: 0.7030 - learning_rate: 5.0000e-04\n",
            "Epoch 3/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8924 - loss: 0.5752 - val_accuracy: 0.8202 - val_loss: 0.6863 - learning_rate: 5.0000e-04\n",
            "Epoch 4/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9098 - loss: 0.5349 - val_accuracy: 0.8621 - val_loss: 0.6493 - learning_rate: 5.0000e-04\n",
            "Epoch 5/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9202 - loss: 0.5017 - val_accuracy: 0.8609 - val_loss: 0.6434 - learning_rate: 5.0000e-04\n",
            "Epoch 6/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9212 - loss: 0.4917 - val_accuracy: 0.8647 - val_loss: 0.6342 - learning_rate: 5.0000e-04\n",
            "Epoch 7/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9260 - loss: 0.4695 - val_accuracy: 0.8679 - val_loss: 0.6189 - learning_rate: 5.0000e-04\n",
            "Epoch 8/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9334 - loss: 0.4516 - val_accuracy: 0.8767 - val_loss: 0.6105 - learning_rate: 5.0000e-04\n",
            "Epoch 9/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9328 - loss: 0.4438 - val_accuracy: 0.8653 - val_loss: 0.6150 - learning_rate: 5.0000e-04\n",
            "Epoch 10/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9318 - loss: 0.4370 - val_accuracy: 0.8755 - val_loss: 0.6093 - learning_rate: 5.0000e-04\n",
            "Epoch 11/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9360 - loss: 0.4240 - val_accuracy: 0.8787 - val_loss: 0.6062 - learning_rate: 5.0000e-04\n",
            "Epoch 12/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9359 - loss: 0.4209 - val_accuracy: 0.8742 - val_loss: 0.6012 - learning_rate: 5.0000e-04\n",
            "Epoch 13/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9346 - loss: 0.4152 - val_accuracy: 0.8818 - val_loss: 0.5749 - learning_rate: 5.0000e-04\n",
            "Epoch 14/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9401 - loss: 0.3984 - val_accuracy: 0.8863 - val_loss: 0.5793 - learning_rate: 5.0000e-04\n",
            "Epoch 15/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9404 - loss: 0.3912 - val_accuracy: 0.8831 - val_loss: 0.5661 - learning_rate: 5.0000e-04\n",
            "Epoch 16/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9388 - loss: 0.3920 - val_accuracy: 0.8806 - val_loss: 0.5838 - learning_rate: 5.0000e-04\n",
            "Epoch 17/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9451 - loss: 0.3744 - val_accuracy: 0.8729 - val_loss: 0.5662 - learning_rate: 5.0000e-04\n",
            "Epoch 18/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9531 - loss: 0.3528 - val_accuracy: 0.8780 - val_loss: 0.5381 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9493 - loss: 0.3393 - val_accuracy: 0.8761 - val_loss: 0.5311 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9507 - loss: 0.3179 - val_accuracy: 0.8774 - val_loss: 0.5205 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9543 - loss: 0.3094 - val_accuracy: 0.8837 - val_loss: 0.5071 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9512 - loss: 0.3086 - val_accuracy: 0.8831 - val_loss: 0.5110 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9490 - loss: 0.3010 - val_accuracy: 0.8818 - val_loss: 0.4999 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9526 - loss: 0.2953 - val_accuracy: 0.8844 - val_loss: 0.4891 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9511 - loss: 0.2974 - val_accuracy: 0.8793 - val_loss: 0.4925 - learning_rate: 1.0000e-04\n",
            "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "Akurasi Training: 95.32%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.97      0.95      0.96      5258\n",
            "      netral       0.92      0.98      0.95      5281\n",
            "     positif       0.97      0.93      0.95      5198\n",
            "\n",
            "    accuracy                           0.95     15737\n",
            "   macro avg       0.95      0.95      0.95     15737\n",
            "weighted avg       0.95      0.95      0.95     15737\n",
            "\n",
            "\n",
            "Akurasi Testing: 89.06%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.88      0.91      0.89      2236\n",
            "      netral       0.86      0.98      0.91      2213\n",
            "     positif       0.95      0.79      0.86      2296\n",
            "\n",
            "    accuracy                           0.89      6745\n",
            "   macro avg       0.90      0.89      0.89      6745\n",
            "weighted avg       0.90      0.89      0.89      6745\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nerawang_sentiment(text, model, vectorizer, is_word2vec=False, tokenizer=None, max_len=100):\n",
        "    cleaned_text = bersihin_text(text)\n",
        "    if is_word2vec:\n",
        "        seq = tokenizer.texts_to_sequences([cleaned_text])\n",
        "        padded = pad_sequences(seq, maxlen=max_len)\n",
        "        pred = model.predict(padded)\n",
        "    else:\n",
        "        tfidf_vec = vectorizer.transform([cleaned_text]).toarray()\n",
        "        pred = model.predict(tfidf_vec)\n",
        "    sentiment = np.argmax(pred, axis=1)[0]\n",
        "    return ['negatif', 'netral', 'positif'][sentiment]\n",
        "\n",
        "sample_text = \"minecraft game nya bisa membangun kreativitas\"\n",
        "\n",
        "print(\"\\nContoh Inference:\")\n",
        "print(f\"Skema 1 : {nerawang_sentiment(sample_text, model1, tfidf)}\")\n",
        "print(f\"Skema 2 : {nerawang_sentiment(sample_text, model2, None, True, tokenizer)}\")\n",
        "print(f\"Skema 3 : {nerawang_sentiment(sample_text, model3, tfidf)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5eACFUk9HQ3",
        "outputId": "c2d7d88a-a010-4537-ee36-ca24c3e7c473"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contoh Inference:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Skema 1 : positif\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Skema 2 : positif\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607ms/step\n",
            "Skema 3 : positif\n"
          ]
        }
      ]
    }
  ]
}